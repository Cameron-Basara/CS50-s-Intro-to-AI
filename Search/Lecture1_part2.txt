CS 50 - Intro to AI - Unit 1 - Search


Lecture 1 Notes:

Terms: 

Agent- entity that perceives and acts on its environment

State- Some configuration of an agent in its environment

Initial State- State which the agent begins - Agent0

Actions- Choices we can make in any given state

Transition model- description of what state we get after we perform some action on an existing state

State space- set of all states we can get from an initial state, given any sequence of actions

Goal test- way to determine if a state is a goal state

Path cost- numerical cost assosiated with a given path

Search problems:

Some initial state -> actions -> transition model -> goal test -> path cost function

Solution- some sequence of actions that brings us from the initial state to the goal state

Optimal solution- lowest path cost sequence

Node - data structure that keep track of : a state - a parent - an action - a path cost

Approach:

Start with a frontier that contains 1 state

Repeat:
	If the frontier is empty, there is no solution

	Remove a node from the frontier

	if the node is the goal, then return the solution

	expand the node, add resulting nodes to the frontier

Approach:

Start with a frontier that contains 1 state

start with an empty explored state

Repeat:

	If the frontier is empty, there is no solution

	Remove a node from the frontier

	if the node is the goal, then return the solution

	add the node to the explored set

	expand the node, add resulting nodes to the frontier if they arent already in the explored set or the frontier


Stack - last in first out data structure

This is depth first search
alwawys explore the deepest node in the tree

Breadth search first - always explores the shallowest node in the tree 

uses a queue

queue - first in first out data structure

uninformed search- search that uses no problem specific knowledge

informed search- search strategies that use knowledge specific to a problem to be able to better solve a problem

greedy best-first search- SA that expands the node that is closest to the goal, as estimated by a heuristic function h(n)

Heuristic function- a function that pases value by some judgement system

Manhattan distance - heuristic that tells you how far it is from the goal, ignoring the walls

A* search - search algorithm that uses greedy best-first search, but adds another factor to the heuristic

A* search -> expands node with lowest value of g(n) + h(n)

where g(n) = cost to reach node, h(n) = estimated cost to goal ; in plain english: How far away i am from the goal and how far have I been to get here (1+16, 2+15, 3+14)

optimal if: h(n) never overestimates the cost, h(n) is consistent

Adversial search - search where I have an objective, and another agent has an objective which is against me

Minimax - min(x), max(o) where one wants to minimize the score, and one wants to maximize the score

Game - S0: initial state
	PLAYER(s): returns which player to move in state s
	ACTION(s): returns legal move in state s
	RESULT(s,a): returns state after action a taken in state s
	TERMINAL(s): checks if state is terminal state
	UTILITY(s): returns the value of the terminal state

Alpha-Beta pruning - keeping track of values and pruning search trees to find the optimal option for you

Depth-limted minimax -  after a certain number of moves, stop considering

Adds an evaluation function - adds expected utility of the game from a given state















	
